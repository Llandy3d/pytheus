{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p> playing with metrics </p> <p> </p>"},{"location":"#introduction","title":"Introduction","text":"<p>pytheus is a modern python library for collecting prometheus metrics built with multiprocessing in mind.</p> <p>Some of the features are:</p> <ul> <li>multiple multiprocess support:<ul> <li>redis backend \u2705</li> <li>Rust powered backend \ud83e\uddea</li> <li>bring your own \u2705</li> </ul> </li> <li>support for default labels value \u2705</li> <li>partial labels value (built in an incremental way) \u2705</li> <li>customizable registry support \u2705</li> <li>registry prefix support \u2705</li> </ul>"},{"location":"#philosophy","title":"Philosophy","text":"<p>Simply put is to let you work with metrics the way you want.</p> <p>Be extremely flexible, allow for customization from the user for anything they might want to do without having to resort to hacks and most importantly offer the same api for single &amp; multi process scenarios, the switch should be as easy as loading a different backend without having to change anything in the code.</p> <ul> <li>What you see is what you get.</li> <li>No differences between <code>singleprocess</code> &amp; <code>multiprocess</code>, the only change is loading a different backend and everything will work out of the box.</li> <li>High flexibility with an high degree of <code>labels</code> control and customization.</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>redis &gt;= 4.0.0 (optional: for multiprocessing)</li> <li>pytheus-backend-rs (optional: for Rust powered multiprocessing \ud83e\udd80)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pytheus\n</code></pre> <p>Optionally if you want to use the Redis backend (for multiprocess support) you will need the redis library: <pre><code>pip install redis\n\n# or everything in one command\npip install pytheus[redis]\n</code></pre></p> <p>If you want to try the Rust based backend (for multiprocess support): <pre><code>pip install pytheus-backend-rs\n</code></pre></p> <p>Tip</p> <p>Try the Rust powered backend! \ud83e\udd80 <pre><code>pip install pytheus-backend-rs\n</code></pre></p>"},{"location":"#example","title":"Example","text":"example.py<pre><code>import time\nfrom flask import Flask, Response\nfrom pytheus.metrics import Histogram\nfrom pytheus.exposition import generate_metrics, PROMETHEUS_CONTENT_TYPE\n\napp = Flask(__name__)\n\nhttp_request_duration_seconds = Histogram(\n    'http_request_duration_seconds', 'documenting the metric..'\n)\n\n@app.route('/metrics')\ndef metrics():\n    data = generate_metrics()\n    return Response(data, headers={'Content-Type': PROMETHEUS_CONTENT_TYPE})\n\n# track time with the context manager\n@app.route('/')\ndef home():\n    with http_request_duration_seconds.time():\n        return 'hello world!'\n\n# alternatively you can also track time with the decorator shortcut\n@app.route('/slow')\n@http_request_duration_seconds\ndef slow():\n    time.sleep(3)\n    return 'hello world! from slow!'\n\napp.run(host='0.0.0.0', port=8080)\n</code></pre> <p>Run the app with <code>python example.py</code> and visit either <code>localhost:8080</code> or <code>localhost:8080/slow</code> and finally you will be able to see your metrics on <code>localhost:8080/metrics</code>!</p> <p>You can also point prometheus to scrape this endpoint and see directly the metrics in there.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This section gives an high level overview of how the library works internally.</p> <p>Note</p> <p>You might be more interested in getting your hands dirty and play with it in the Quickstart.</p>"},{"location":"architecture/#metrics","title":"Metrics","text":"<p>For the metrics we have a <code>_MetricCollector</code> class that is the core logic for a specific named metric. It will do label validation, it will have the metric name and most importantly it will keep track of all the Child instances so that when <code>collect()</code> is called it will retrieve all the correct samples from all the possible observable Childrens.</p> <p>Subclasses of <code>_Metric</code> (ex. <code>Counter</code>) are what prometheus calls Child, a cacheable instance that provides access to a labeled dimension metric. This means that you can have as many instances of <code>_Metric</code> as you want for any combination of labels value. Getting one of this instances is achieved by calling <code>labels()</code>. Beside giving you access to labeled dimensions:   - it will add them automatically to the <code>_MetricCollector</code>   - it will know if itself is observable   - it will provide a <code>collect()</code> to retrieve its <code>Sample</code></p>"},{"location":"architecture/#registry","title":"Registry","text":"<p>The <code>CollectorRegistry</code> class is the default global collector that will keep track of all the created metrics and it's the object that will work on collecting all the samples that possibly can be used for exposing those metrics. A <code>_MetricCollector</code> should be able to <code>register()</code> itself into it. The <code>collect()</code> method will hold the logic for collecting all the metrics samples and should combine each metric name with the samples <code>suffixes</code> if present before exposing them. (This last bit might not be part of the registry directly, but of the piece that will actually work on formatting the metrics for exposure)</p> <p>The <code>RegistryProxy</code> will be the actual global object in the library that will just proxy all the registry calls directly to the set registry. This object would allow to swap the registry with the prefered user one during initialization with a <code>set_registry()</code> method, for example an <code>PrefixedCollectorRegistry</code> that would add a common prefix to each metric name.</p> <p>Note</p> <p>The prefix functionality is included in the provided <code>CollectorRegistry</code>.</p>"},{"location":"architecture/#backend","title":"Backend","text":"<p>This is well explained in the Backend / Multiprocessing section of the documentation.</p>"},{"location":"backend/","title":"Backend / Multiprocessing","text":"<p>This is an important part of the library designed to make the switch from single process to multi process monitoring extremely simple without requiring changes from the user beside a configuration.</p> <p><code>Backend</code> is a protocol used for each value of a metric, meaning that when you create a <code>Counter</code> for example, its actual value will be an instance of a <code>Backend</code> that will handle the logic for that value. For single process it could be a mutex based implementation while for multi process it could be a redis backed one.</p> <p>Regardless of backend used, if you want to increment the <code>Counter</code> you would call the same <code>inc()</code> method, so that you can easily switch between single and multi process with ease.</p> <p>Note</p> <p>One of my goals for this library is to have the interface be the same between single &amp; multi process, meaning that the features supported should always handle both cases.</p>"},{"location":"backend/#backend-protocol","title":"Backend Protocol","text":"<p>The backend protocol is as follows:</p> <pre><code>class Backend(Protocol):\n    def __init__(\n        self,\n        config: BackendConfig,\n        metric: \"_Metric\",\n        ,\n    ) -&gt; None:\n        ...\n\n    def inc(self, value: float) -&gt; None:\n        ...\n\n    def dec(self, value: float) -&gt; None:\n        ...\n\n    def set(self, value: float) -&gt; None:\n        ...\n\n    def get(self) -&gt; float:\n        ...\n</code></pre> <p>When creating a custom backend the <code>__init__</code> method has to accept:</p> <ul> <li><code>config: BackendConfig</code>: possible configuration values for initializing it</li> <li><code>metric: _Metric</code>: the metric that is creating the instance of this backend for a value</li> <li><code>histogram_bucket: str | None</code>: bucket name in case it's instanced for an histogram</li> </ul> <p>You don't have to use these values in your implementation, they just need to be accepted.</p> <p>Note</p> <p><code>BackendConfig</code> is of type <code>dict[str, Any]</code></p> <p><code>_Metric</code> is any of <code>Counter</code>, <code>Gauge</code>, <code>Histogram</code></p> <p>Tip</p> <p>It's possible that you want to initialize your custom backend or there are one time steps that you want to happen on import.</p> <p>To achieve that you can use the class method hook called <code>_initialize</code> that accepts a <code>BackendConfig</code> parameter.</p> <pre><code>@classmethod\ndef _initialize(cls, config: \"BackendConfig\") -&gt; None:\n    # initialization steps\n</code></pre> <p>Tip</p> <p>A <code>Backend</code> can have a <code>_generate_samples</code> class method that would be used when generating the metrics instead of going over every metric get method.</p> <p>This allows for flexibility in implementing faster implementations, an example is the use of pipelines in the <code>MultiProcessRedisBackend</code></p>"},{"location":"backend/#default-backend","title":"Default Backend","text":"<p>The default backend used is a <code>SingleProcessBackend</code>. A thread-safe in-memory implementation that makes use of <code>threading.Lock</code>.</p> <pre><code>from pytheus.metrics import Counter\n\ncounter = Counter('cache_hit_total', 'description')\nprint(counter._metric_value_backend.__class__)\n# &lt;class 'pytheus.backends.base.SingleProcessBackend'&gt;\n</code></pre>"},{"location":"backend/#multiprocess-backend","title":"Multiprocess Backend","text":"<p>The library also includes a redis based implementation of the backend for supporting multi process python services: <code>MultiProcessRedisBackend</code>.</p> <p>This makes use of the <code>INCRBYFLOAT</code> &amp; <code>HINCRBYFLOAT</code> redis operations that are <code>ATOMIC</code> and as redis is single-threaded it means that even if multiple clients try to update the same value we will end up with the correct one.</p> <p>Tip</p> <p>When using the <code>MultiProcessRedisBackend</code> you will want to have a separate redis server o database specified per service you are monitoring. If you don't there is a risk that a metric with the same name on one service might overwrite one in another service.</p> <p>Warning</p> <p>DEPRECATED</p> <p>If you plan to share the same redis server for multiple services, you can configure a prefix that will be added to all the stored metrics with the <code>key_prefix</code>.</p> <p>For example: <code>{\"host\": \"127.0.0.1\", \"port\": 6379, \"key_prefix\": \"serviceprefix\"}</code></p> <p>Tip</p> <p>You can configure the time keys will stay alive in Redis by passing the key <code>expire_key_time</code> in the configuration. This is 1 hour by default and it gets refreshed everytime an operation is done against the key, for example when it gets scraped.</p> <pre><code># 5 min\nload_backend(MultiProcessRedisBackend, {\"expire_key_time\": 300})\n</code></pre>"},{"location":"backend/#loading-a-different-backend","title":"Loading a different Backend","text":"<p>To load a different backend you can make use of the <code>load_backend</code> function, for example for loading the <code>MultiProcessRedisBackend</code>:</p> <pre><code>from pytheus.backends import load_backend\nfrom pytheus.backends.redis import MultiProcessRedisBackend\n\nload_backend(\n    backend_class=MultiProcessRedisBackend,\n    backend_config={\"host\": \"127.0.0.1\", \"port\": 6379},\n)\n</code></pre> <p>now when you create a metric it will use the configured backend:</p> <pre><code>counter = Counter('cache_hit_total', 'description')\nprint(counter._metric_value_backend.__class__)\n# &lt;class 'pytheus.backends.redis.MultiProcessRedisBackend'&gt;\n</code></pre> <p>Warning</p> <p>This operation should be done before you create your metrics as an initialization step.</p> <p>Tip</p> <p><code>load_backend()</code> it's called automatically when you import the <code>pytheus</code> library and it supports environment variables to configure which backend to use and the config, so you can just set them without having to call the function yourself:</p> <ul> <li><code>PYTHEUS_BACKEND_CLASS</code>: class to import, for example <code>pytheus.backends.redis.MultiProcessFileBackend</code></li> <li><code>PYTHEUS_BACKEND_CONFIG</code>: path to a <code>json</code> file containing the config</li> </ul> <p>Note</p> <p>The function definition is: <pre><code>def load_backend(\n    backend_class: type[Backend] | None = None,\n    backend_config: BackendConfig | None = None,\n) -&gt; None:\n</code></pre></p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#061","title":"0.6.1","text":"<ul> <li>fix Mypy not recognizing type annotations</li> <li>fix <code>labels</code> method returning <code>_Metrics</code> instead of <code>Self</code></li> </ul>"},{"location":"changelog/#060","title":"0.6.0","text":"<ul> <li>python 3.13 support</li> </ul>"},{"location":"changelog/#050","title":"0.5.0","text":"<ul> <li> <p>New improved implementation of <code>_generate_samples</code> for <code>MultiProcessRedisBackend</code>:</p> <ul> <li>Improved performance for generating metrics</li> <li>Fixed a bug where on multiprocess on some scrapes a metric wouldn't be picked up due to missing local collector child</li> <li>Now on labeled metrics, the labels/value combination will be initialized to 0 if it doesn't exists (before only the key would be initialized)</li> </ul> </li> <li> <p>Redis keys expire time can now be configured via the <code>expire_key_time</code> config passed when loading the backend</p> </li> <li> <p>Experimental <code>prometheus_client</code> patching in-place to quickly test the library on existing codebases</p> </li> <li> <p>DEPRECATED: <code>key_prefix</code> for redis configuration. Prefer a side-car container or a separate redis/redis db per service.</p> </li> </ul>"},{"location":"changelog/#040","title":"0.4.0","text":"<ul> <li>python 3.12 support</li> </ul>"},{"location":"changelog/#030","title":"0.3.0","text":"<ul> <li>fix <code>generate_metrics</code> to handle custom collectors</li> </ul>"},{"location":"changelog/#020","title":"0.2.0","text":"<ul> <li><code>kwargs</code> (keyword arguments) support for the <code>labels()</code> method.    <pre><code>metric = Counter(\"counter\", \"desc\", required_labels=[\"method\"])\n\n# dict approach\nmetric.labels({\"method\": \"POST\"})\n\n# kwargs approach\nmetric.labels(method=\"POST\")\n</code></pre></li> </ul>"},{"location":"changelog/#010","title":"0.1.0","text":"<ul> <li>asgi middleware for <code>FastAPI</code> support</li> </ul>"},{"location":"changelog/#0016","title":"0.0.16","text":"<ul> <li>add <code>Summary</code> metric type</li> </ul>"},{"location":"changelog/#0015","title":"0.0.15","text":"<ul> <li>add support for python <code>3.8</code> &amp; <code>3.9</code></li> </ul>"},{"location":"changelog/#0014","title":"0.0.14","text":"<ul> <li>decorator support for async functions</li> </ul>"},{"location":"changelog/#0013","title":"0.0.13","text":"<ul> <li>Support for <code>_generate_samples</code> in Backend</li> <li>MultiProcessRedisBackend:<ul> <li>Improved scraping performance via pipelining</li> </ul> </li> </ul>"},{"location":"changelog/#0012","title":"0.0.12","text":"<ul> <li>Escape <code>help</code> &amp; <code>label_values</code> during exposition</li> <li>MultiProcessRedisBackend:<ul> <li>Add <code>key_prefix</code> config to specify stored key name prefix.</li> </ul> </li> </ul>"},{"location":"changelog/#0011","title":"0.0.11","text":"<ul> <li>MultiProcessRedisBackend:<ul> <li>handle <code>get</code> with unexpected key deletion</li> </ul> </li> </ul>"},{"location":"changelog/#0010","title":"0.0.10","text":"<ul> <li>MultiProcessRedisBackend:<ul> <li>initialize <code>key</code> on redis on instantiation</li> <li>update expiry when retrieving a key</li> </ul> </li> </ul>"},{"location":"changelog/#009","title":"0.0.9","text":"<ul> <li>The <code>inf</code> histogram bucket label will now be exported as <code>+Inf</code> in line with the other clients</li> </ul>"},{"location":"changelog/#008","title":"0.0.8","text":"<ul> <li><code>Histogram</code> now passes custom buckets to child instances</li> </ul>"},{"location":"changelog/#007","title":"0.0.7","text":"<ul> <li>fix <code>MetricType</code> string representation for python 3.11</li> </ul>"},{"location":"changelog/#006","title":"0.0.6","text":"<p>First release ready to be tested \ud83c\udf89</p> <p>Redis multiprocess backend support still experimental.</p>"},{"location":"counter/","title":"Counter","text":"<p>A <code>Counter</code> is a metric that only increases in value, it could also reset to 0 for example in case of a service restart.</p> <p>As the name suggests it's useful to count things, for example cache hits, number of connections or even count exceptions.</p> <p>Tip</p> <p>If you need a metric that can go up and down in value, checkout the Gauge.</p>"},{"location":"counter/#usage","title":"Usage","text":"<p>To create a <code>Counter</code> import it and instantiate:</p> <pre><code>from pytheus.metrics import Counter\n\ncache_hit_total = Counter(name='cache_hit_total', description='number of times the cache got it')\n</code></pre>"},{"location":"counter/#increment-the-value","title":"Increment the value","text":"<p>Now it's possible to increment the count by calling the <code>inc()</code>:</p> <pre><code># increases value by 1\ncache_hit_total.inc()\n</code></pre> <p>it is also possible to specify by which amount to increase:</p> <pre><code># increases value by 7\ncache_hit_total.inc(7)\n</code></pre> <p>Warning</p> <p><code>inc()</code> accepts only positive values as counters cannot decrease.</p>"},{"location":"counter/#count-exceptions","title":"Count Exceptions","text":"<p>As counters are a good fit for counting exceptions there are some nicities included, you can count exceptions within a <code>with</code> statement with <code>count_exceptions</code>:</p> <p>Note</p> <p>the following examples assume a <code>Counter</code> called <code>counter</code>.</p> <pre><code>with counter.count_exceptions():\n    raise ValueError  # increases counter by 1\n</code></pre> <p>It is also possible to specify which <code>Exceptions</code> to count:</p> <pre><code>with counter.count_exceptions((IndexError, ValueError)):\n    raise KeyError. # does not increase as it's not included in the list\n</code></pre> <p>Tip</p> <p><code>count_exceptions</code> accepts an <code>Exception</code> or a tuple of exceptions.</p>"},{"location":"counter/#as-a-decorator","title":"As a Decorator","text":"<p>When used as a decorator the <code>Counter</code> will count exceptions, syntactic sugar to <code>count_exceptions</code>:</p> <pre><code>@counter\ndef my_func():\n    raise ValueError  # increases counter by 1 when it raises\n</code></pre> <p>you are still able to specify which exceptions you want to count:</p> <pre><code>@counter(exceptions=(IndexError, ValueError))\ndef my_func():\n    raise KeyError  # won't increase when raised as it's not in the list\n</code></pre>"},{"location":"custom_collector/","title":"Custom Collector","text":"<p>It is possible to use a custom collector in cases you can't directly instrument the code. You will need to inherit from <code>CustomCollector</code> and define the collect method. Also be sure to disable the automatic registering of a newly created metric into the default registry.</p> <pre><code>from pytheus.metrics import Counter, CustomCollector\nfrom pytheus.registry import REGISTRY\n\nclass MyCustomCollector(CustomCollector):\n    def collect():\n        # note that we are disabling automatic registering\n        counter = Counter('name', 'desc', registry=None)\n        counter.inc()\n        yield counter\n\nREGISTRY.register(MyCustomCollector())\n</code></pre> <p>Note</p> <p>If one of the yield metrics have a name already registered with the registry you are trying to register to, the custom collector will be ignored.</p>"},{"location":"fastapi/","title":"Tutorial / FastAPI","text":"<p>Warning</p> <p>[WIP tutorial]</p> <p>The library offers a middleware that can be added to a FastAPI project to automatically get some metrics making it dead simple to get useful information at a low effort cost.</p>"},{"location":"fastapi/#the-middleware","title":"The middleware","text":"<p>The <code>PytheusMiddlewareASGI</code> when added to the project will collect three metrics:</p> <ul> <li><code>http_request_duration_seconds</code>: the duration of the http request</li> <li><code>http_request_size_bytes</code>: size in bytes of the request</li> <li><code>http_response_size_bytes</code>: size in bytes of the response</li> </ul> <p>These will have three labels: <code>method</code>, <code>path</code> &amp; <code>status_code</code> allowing for a lot of flexibility on observing your system. For example seeing the duration of <code>GET</code> requests to the <code>/api</code> path with a <code>status_code</code> of <code>200</code>.</p> <p>Note</p> <p><code>http_request_size_bytes</code> for now depends on the availability of the <code>Content-Length</code> header in the request.</p>"},{"location":"fastapi/#using-the-middleware","title":"Using the middleware","text":"<p>In the file where you are creating your FastAPI application import and add the middleware:</p> <pre><code>from fastapi import FastAPI\nfrom pytheus.middleware import PytheusMiddlewareASGI\n\n\napp = FastAPI()\napp.add_middleware(PytheusMiddlewareASGI)\n</code></pre> <p>that will take care of collecting the metrics!</p> <p>Now, we just need to decide where to expose the metrics, this is usually a <code>/metrics</code> endpoint:</p> <pre><code>from fastapi.responses import PlainTextResponse\nfrom pytheus.exposition import generate_metrics\n\n\n@app.get('/metrics', response_class=PlainTextResponse)\nasync def pytheus_metrics():\n    return generate_metrics()\n</code></pre> <p>That's it!</p> <p>If you visit the <code>/metrics</code> endpoint twice you will able to see metrics for that request:</p> <pre><code># HELP http_request_duration_seconds duration of the http request\n# TYPE http_request_duration_seconds histogram\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"0.005\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"0.01\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"0.025\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"0.05\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"0.1\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"0.25\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"0.5\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"1\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"2.5\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"5\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"10\"} 1.0\nhttp_request_duration_seconds_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"+Inf\"} 1.0\nhttp_request_duration_seconds_sum{method=\"GET\",route=\"/metrics\",status_code=\"200\"} 0.0014027919969521463\nhttp_request_duration_seconds_count{method=\"GET\",route=\"/metrics\",status_code=\"200\"} 1.0\n# HELP http_request_size_bytes http request size\n# TYPE http_request_size_bytes histogram\n# HELP http_response_size_bytes http response size\n# TYPE http_response_size_bytes histogram\nhttp_response_size_bytes_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"10.0\"} 0.0\nhttp_response_size_bytes_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"100.0\"} 0.0\nhttp_response_size_bytes_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"1000.0\"} 1.0\nhttp_response_size_bytes_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"10000.0\"} 1.0\nhttp_response_size_bytes_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"100000.0\"} 1.0\nhttp_response_size_bytes_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"1000000.0\"} 1.0\nhttp_response_size_bytes_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"10000000.0\"} 1.0\nhttp_response_size_bytes_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"100000000.0\"} 1.0\nhttp_response_size_bytes_bucket{method=\"GET\",route=\"/metrics\",status_code=\"200\",le=\"+Inf\"} 1.0\nhttp_response_size_bytes_sum{method=\"GET\",route=\"/metrics\",status_code=\"200\"} 296.0\nhttp_response_size_bytes_count{method=\"GET\",route=\"/metrics\",status_code=\"200\"} 1.0\n</code></pre> <p>Note</p> <p>You will need to hit <code>/metrics</code> twice because the first time on a fresh start you won't have metrics. On the second request you will see metrics from the previous request.</p>"},{"location":"fastapi/#todo-prometheus-scraping-target-grafana-dashboard","title":"TODO: prometheus scraping target &amp; grafana dashboard","text":""},{"location":"gauge/","title":"Gauge","text":"<p>The <code>Gauge</code> is a metric whose value can go up and down. It's useful to measure things like memory usage or temperatures for example.</p>"},{"location":"gauge/#usage","title":"Usage","text":"<pre><code>from pytheus.metrics import Gauge\n\ngauge = Gauge('room_temperature', 'temperature in the living room')\n</code></pre>"},{"location":"gauge/#increment","title":"Increment","text":"<p>It is possible to increment by 1 by calling <code>inc()</code>:</p> <pre><code># increase by 1\ngauge.inc()\n</code></pre> <p>it's also possible to specify the amount:</p> <pre><code># increase by 7\ngauge.inc(7)\n</code></pre>"},{"location":"gauge/#decrement","title":"Decrement","text":"<p>As for incrementing we can call <code>dec()</code> to decrement the value by 1:</p> <pre><code># decrease by 1\ngauge.dec()\n</code></pre> <p>or we can specify the amount we want to decrement:</p> <pre><code># decrease by 7\ngauge.dec(7)\n</code></pre>"},{"location":"gauge/#set-a-value","title":"Set a value","text":"<p>A <code>Gauge</code> value can be set directly with the <code>set()</code> method:</p> <pre><code>gauge.set(3)\n</code></pre> <p>and as an utility you can also set it to the current time (unix timestamp):</p> <pre><code>gauge.set_to_current_time()\n</code></pre>"},{"location":"gauge/#track-progress","title":"Track progress","text":"<p>You can use the <code>track_inprogress</code> context manager to track progress of things, each time the context manager is entered it will increase the gauge value by 1 and each time it exits it will decrease that value:</p> <pre><code>with gauge.track_inprogress():\n    do_something()\n</code></pre>"},{"location":"gauge/#track-time","title":"Track time","text":"<p>A <code>Gauge</code> can time a piece of code, it will set the value to the duration in seconds if you call the <code>time()</code> context manager:</p> <pre><code>with gauge.time():\n    do_something()\n</code></pre>"},{"location":"gauge/#as-a-decorator","title":"As a Decorator","text":"<p>When used as a decorator the <code>Gauge</code> will time the piece of code, syntactic sugar to the <code>time()</code> context manager:</p> <pre><code>@gauge\ndef do_something():\n    ...\n</code></pre> <p>It is also possible to pass the <code>track_inprogress</code> flag to make the decorator work as syntactic sugar for the <code>track_inprogress</code> context manager:</p> <pre><code>@gauge(track_inprogress=True)\ndef do_something():\n    ...\n</code></pre>"},{"location":"histogram/","title":"Histogram","text":"<p>An <code>Histogram</code> is a metric that samples observations and counts them in configurable buckets. It is useful to measure things like request durations or response sizes.</p> <p>When scraped an histogram produces multiple time series:</p> <ul> <li>cumulative counters for the observation buckets (ex. <code>http_request_duration_seconds{le=\"1\"}</code>)</li> <li>total sum of observed values (ex. <code>http_request_duration_seconds_sum</code>)</li> <li>count of events that have been observed (ex. <code>http_request_duration_seconds_count</code>)</li> </ul> <p>Tip</p> <p>You can use the <code>histogram_quantile</code> function in prometheus to calculate quantiles, for examples if you have an <code>http_request_duration_seconds</code> histogram you could see how an endpoint is doing in the 95th percentile.</p> <p>Tip</p> <p>For more information on how to use histograms check the prometheus docs</p>"},{"location":"histogram/#usage","title":"Usage","text":"<pre><code>from pytheus.metrics import Histogram\n\nhistogram = Histogram(name=\"http_request_duration_seconds\", description=\"My description\")\n</code></pre>"},{"location":"histogram/#configure-buckets","title":"Configure Buckets","text":"<p>By default an <code>Histogram</code> will be created with the following buckets: <pre><code>(.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10)\n</code></pre></p> <p>Default buckets are tailored to broadly measure the response time (in seconds) of a network, you most likely will want to pass a customized list and you can with the <code>buckets</code> parameter when creating the histogram:</p> <pre><code>customized_buckets = (0.1, 0.3, 2, 5)\nhistogram = Histogram(\n    name=\"http_request_duration_seconds\",\n    description=\"My description\",\n    buckets=customized_buckets,\n)\n</code></pre> <p>Note</p> <p><code>buckets</code> accepts a <code>Sequence[float]</code>.</p> <p>Note</p> <p>The <code>+Inf</code> bucket will be added automatically, this is <code>float('inf')</code> in python</p>"},{"location":"histogram/#observe-a-value","title":"Observe a value","text":"<p>To observe a value you can call the <code>observe()</code> method:</p> <pre><code>histogram.observe(0.4)\n</code></pre>"},{"location":"histogram/#track-time","title":"Track time","text":"<p>You can track time with the <code>time()</code> context manager, it will track the duration in seconds:</p> <pre><code>with histogram.time():\n    do_something()\n</code></pre>"},{"location":"histogram/#as-a-decorator","title":"As a Decorator","text":"<p>When used as a decorator the <code>Histogram</code> will time the piece of code, syntactic sugar to the <code>time()</code> context manager:</p> <pre><code>@histogram\ndef do_something():\n    ...\n</code></pre>"},{"location":"labels/","title":"Labels","text":"<p>With labels you can differentiate what you are measuring, for example for an <code>http_request_duration_seconds</code> metric you might want to differentiate between <code>GET</code> or <code>POST</code> requests while using the same metric.</p> <p>Warning</p> <p>Every new value for a label key represents a new time serie, so you should try to avoid labels that measure things with high cardinality, for example a path that has no specific ending: <code>/users/{id}</code> {here id is variable depending on the request}.</p> <p>In that case it would be better to observe the metric replacing the last part of the endpoint with a fixed value so that you still get useful information: <code>/users/:id</code></p>"},{"location":"labels/#creating-a-metric-with-a-label","title":"Creating a metric with a label","text":"<p>To create a metric that requires labels we will use the <code>required_labels</code> parameter when creating our metric. It's a list of strings. To show how to use a label, let's create a <code>Counter</code> that will accept a <code>method</code> label:</p> <p>Note</p> <p><code>required_labels</code>:  accepts a <code>Sequence[str]</code></p> <pre><code>from pytheus.metrics import Counter\n\npage_hit_total = Counter(\n    'page_hit_total',\n    'number of time this page got it',\n    required_labels=['method'],\n)\n</code></pre> <p>Since our new metric has atleast a label required, if you try to observe it, it will fail:</p> <pre><code>page_hit_total.inc()\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/llandy/dev/pytheus/pytheus/metrics.py\", line 283, in inc\n    self._raise_if_cannot_observe()\n  File \"/Users/llandy/dev/pytheus/pytheus/metrics.py\", line 198, in _raise_if_cannot_observe\n    raise UnobservableMetricException\npytheus.exceptions.UnobservableMetricException\n</code></pre> <p>A metric is considered \"unobservable\" if you try to observe it without passing the labels that were defined as required. The correct way to observe it for example with the <code>method</code> label mapped to <code>GET</code> would be:</p> <pre><code>page_hit_total.labels({'method': 'GET'}).inc()\n</code></pre> <p>Note</p> <p>The <code>labels</code> method accepts <code>dict[str, str]</code> where the key of the dictionary is the label name and the value is the value of the label. Alternatively you can use keyword arguments.</p> <p>Tip</p> <p>You can use <code>kwargs</code> instead of a <code>dict</code> like this:</p> <pre><code>page_hit_total.labels(method='GET'}).inc()\n</code></pre> <p>we can do the same for the label mapped to the value <code>POST</code> and the metrics created would be:</p> <pre><code>page_hit_total.labels({'method': 'POST'}).inc(3)\n</code></pre> the created time series<pre><code>page_hit_total{method=\"GET\"} 1.0\npage_hit_total{method=\"POST\"} 3.0\n</code></pre> <p>this allows us to check the <code>page_hit_total</code> metric in prometheus, or if we want specifics we can check the values for a specific label like <code>page_hit_total{method=\"GET\"}</code>!</p>"},{"location":"labels/#caching-the-instance-with-a-set-label","title":"Caching the instance with a set label","text":"<p>Calling everytime the <code>labels()</code> method is tiresome, so it's possible to assign the value it returns to a new variable, it returns an instance of the metric class you used with the label already set!</p> <pre><code>page_hit_total_with_get = page_hit_total.labels({'method': 'GET'})\n</code></pre> <p>then we can observe it directly:</p> <pre><code>page_hit_total_with_get.inc()\n</code></pre> <p>Note</p> <p>This works because when you create a metric, for example with <code>Counter</code>, the object you receive is like a \"view\" into the metric.</p> <p>Under the hood there is an <code>_MetricCollector</code> class that handles all the logic for all the labels combinations, while the object you interact with is a <code>_Metric</code> object.</p> <p>When you call the <code>labels()</code> method on a metric object you will receive a new <code>_Metric</code> instance with the labels set. This allows for partial labels!</p>"},{"location":"labels/#default-labels","title":"Default labels","text":"<p>Default labels allow you to define a value that will be used by default for a set of required labels.</p> <p>For example you might want to measure the duration of http requests from different services with the same metric name, it would be annoying having to set that label everytime so we can just define a default to use and possibly overwrite it when needed!</p> <pre><code>http_request_duration_seconds = Histogram(\n    'http_request_duration_seconds',\n    'documenting the metric..',\n    required_labels=['service'],\n    default_labels={'service': 'main_service'},\n)\n</code></pre> <p>we can observe it directly even if there is a required label since it's configured with a default value:</p> <pre><code>http_request_duration_seconds.observe(1)\n</code></pre> <p>or we can override the default value:</p> <pre><code>http_request_duration_seconds_side_service = http_request_duration_seconds.labels(\n    {'service': 'side_service'}\n)\n</code></pre> <p>Note</p> <p><code>default_labels</code> takes a <code>dict[str, str]</code></p>"},{"location":"labels/#partial-labels","title":"Partial labels","text":"<p>With the <code>labels()</code> method returning a new instance of the metric class, it's possible to have instances that are not observable but have some defined values. This would be useful if there are a set of labels that you want to share and just a few remaining ones are different.</p> <p>Let's say that we have two systems <code>a</code> &amp; <code>b</code>, and we want to meausure how many times a cache gets hit plus we want a label telling us the origin of the call, we can define the metric like this:</p> <pre><code>cache_hit_count_total = Counter(\n    'cache_hit_count_total',\n    'number of time the cache got it',\n    required_labels=['system', 'origin'],\n)\n\ncache_hit_count_total_system_a = cache_hit_count_total.labels({'system': 'a'})\ncache_hit_count_total_system_b = cache_hit_count_total.labels({'system': 'b'})\n</code></pre> <p>we defined two partial metrics with a label set for system <code>a</code> and one for system <code>b</code>. These are partial labels because if you try to observe them they will raise an error since not all the required labels are set.</p> <p>We can then use them to observe the metrics avoiding the repetition of having to define the system each time:</p> <pre><code>cache_hit_count_total_system_a.labels({'origin': 'function_a'}).inc()\ncache_hit_count_total_system_a.labels({'origin': 'function_b'}).inc()\n\ncache_hit_count_total_system_b.labels({'origin': 'function_a'}).inc()\n</code></pre> <p>As you can see there could be some repetition from the previous example if the metric would be used in different places calling <code>labels()</code> again and again is annoying so we can create a new variable to hold a metric with also the <code>origin</code> label set:</p> <pre><code>cache_hit_count_total_system_a_with_origin.labels({'origin': 'function_a'})\n\ncache_hit_count_total_system_a_with_origin.inc()\n</code></pre> <p>This is the flexibility of partial labels, you can \"cache\" into variables your metric like you want and build upon it for what it makes sense for your use case.</p> <p>Note</p> <p>Partial labels are built in an incremental way, meaning that each new instance returned when calling <code>labels(...)</code> will return an instance with the previous labels set.</p> <p>If you call the <code>labels({})</code> method with an empty dicy <code>{}</code> it will just return the same instance.</p>"},{"location":"quickref/","title":"Quickref","text":""},{"location":"quickref/#partial-labels","title":"Partial Labels","text":"<pre><code>from pytheus.metrics import Counter\n\n# without labels\nmy_metric = Counter('metric_name', 'desc')\nmy_metric.inc()  # example for counter\n\n# with labels\nmy_metric = Counter('metric_name', 'desc', required_labels=['req1', 'req2'])\n\nmy_metric.labels({'req1': '1', 'req2': '2'}).inc()  # you can pass all the labels at once\npartial_my_metric = my_metric.labels({'req1': '1'})  # a cacheable object with one of the required labels already set\nobservable_my_metric = partial_my_metric.labels({'req2': '2'})  # finish setting the remaining values before observing\nobservable_my_metric.inc()\n\n# alternatively to using a `dict` you can use `kwargs` (keyword arguments)\nmy_metric.labels(req1='1', req2='2')\n</code></pre>"},{"location":"quickref/#default-labels","title":"Default Labels","text":"<pre><code>from pytheus.metrics import Counter\n\n# with default labels\nmy_metric = Counter('metric_name', 'desc', required_labels=['req1', 'req2'], default_labels={'req2': 'me_set!'})\n\nmy_metric.labels({'req1': '1'}).inc()  # as we have req2 as a default label we only need to set the remaining labels for observing\nmy_metric.labels({'req1': '1', 'req2': '2'})  # you can still override default labels!\n</code></pre>"},{"location":"quickref/#counter","title":"Counter","text":"<pre><code>from pytheus.metrics import Counter\n\ncounter = Counter(name=\"my_counter\", description=\"My description\")\n\n# increase by 1\ncounter.inc()\n\n# increase by x\ncounter.inc(7)\n\n# it is possible to count exceptions\nwith counter.count_exceptions():\n    raise ValueError  # increases counter by 1\n\n# you can specify which exceptions to watch for\nwith counter.count_exceptions((IndexError, ValueError)):\n    raise ValueError. # increases counter by 1\n\n# it is possible to use the counter as a decorator as a shortcut to count exceptions\n@counter\ndef test():\n    raise ValueError  # increases counter by 1 when called\n\n# specifying which exceptions to look for also works with the decorator\n@counter(exceptions=(IndexError, ValueError))\ndef test():\n    raise ValueError  # increases counter by 1 when called\n</code></pre>"},{"location":"quickref/#gauge","title":"Gauge","text":"<pre><code>from pytheus.metrics import Gauge\n\ngauge = Gauge(name=\"my_gauge\", description=\"My description\")\n\n# increase by 1\ngauge.inc()\n\n# increase by x\ngauge.inc(7)\n\n# decrease by 1\ngauge.dec()\n\n# set a specific value\ngauge.set(7)\n\n# set to current unix timestamp\ngauge.set_to_current_time()\n\n# it is possible to track progress so that when entered increases the value by 1, and when exited decreases it\nwith gauge.track_inprogress():\n    do_something()\n\n# you can also time a piece of code, will set the duration in seconds to value when exited\nwith gauge.time():\n    do_something()\n\n# tracking time can also be done as a decorator\n@gauge\ndef do_something():\n    ...\n\n# tracking progress is also available via decorator with a flag\n@gauge(track_inprogress=True)\ndef do_something():\n    ...\n</code></pre>"},{"location":"quickref/#histogram","title":"Histogram","text":"<pre><code>from pytheus.metrics import Histogram\n\nhistogram = Histogram(name=\"my_histogram\", description=\"My description\")\n# by default it will have the following buckets: (.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10)\n# note: the +Inf bucket will be added automatically, this is float('inf') in python\n\n# create a histogram specifying buckets\nhistogram = Histogram(name=\"my_histogram\", description=\"My description\", buckets=(0.2, 1, 3))\n\n# observe a value\nhistogram.observe(0.4)\n\n# you can also time a piece of code, will set the duration in seconds to value when exited\nwith histogram.time():\n    do_something()\n\n# tracking time can also be done as a decorator\n@histogram\ndef do_something():\n    ...\n</code></pre>"},{"location":"quickref/#summary","title":"Summary","text":"<pre><code>from pytheus.metrics import Summary\n\nsummary = Summary(name=\"my_summary\", description=\"My description\")\n\n# observe a value\nsummary.observe(0.4)\n\n# you can also time a piece of code, will set the duration in seconds to value when exited\nwith summary.time():\n    do_something()\n\n# tracking time can also be done as a decorator\n@summary\ndef do_something():\n    ...\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Here we will build piece by piece the introductory example while explaining what the different pieces are doing, we will end with loading the redis backend where you will see that it's a matter of just loading a different backend with a function call :)</p> <p>We will be using Flask to create a small api service where we monitor the time it takes for an endpoint to respond and we will have a slow one on purpose so that we have some different data to query when scraped by prometheus!</p> <p>Note</p> <p>if you download the source repo for the pytheus project, you can use the included <code>docker-compose.yaml</code> file to spin up a redis &amp; prometheus instance already configured that will pick up your metrics so that you can interactively explore them!</p> <p>Or you can always configure prometheus yourself to scrape the <code>/metrics</code> endpoint :)</p>"},{"location":"quickstart/#metrics","title":"Metrics","text":"<p>Metrics are numeric measurements, represented by time-series data (<code>timestamp - float value</code>) that you observe changing over time.</p> <p>They become useful to monitor things like the time it takes for an endpoint to return a response, to count the number of connections overtime or to monitor active queries for example.</p> <p>The available metrics types in the library are:</p> <ul> <li>Counter</li> <li>Gauge</li> <li>Histogram</li> </ul> <p>Note</p> <p>I'm leaving out <code>Summary</code>, the last type supported by prometheus, for now to leave it as a ticket for someone to contribute to :)</p> <p>You can import and create a metric like this:</p> <pre><code>from prometheus.metrics import Counter\n\npage_hit_total = Counter('page_hit_total', 'documentation string')\n</code></pre> <p>to observe it in the specific case of a <code>Counter</code> you would increase it:</p> <pre><code>page_hit_total.inc()\n# or specify the amount\npage_hit_total.inc(2)\n</code></pre>"},{"location":"quickstart/#registry","title":"Registry","text":"<p>The registry it's an object that contains the metrics you created. By default when you create a metric from the appropriate class it will be automatically registered in the global <code>REGISTRY</code> object.</p> <p>This global object is used to generate the text that prometheus can scrape and will handle logic checks to confirm that you are not duplicating metrics name for example. You can access the global registry object by importing it:</p> <pre><code>from pytheus.registry import REGISTRY\n</code></pre> <p>Sometimes you might want to register a metric directly (for example when creating a custom collector) and you can do it like this:</p> <pre><code>REGISTRY.register(mycollector)\n</code></pre> <p>Note</p> <p><code>REGISTRY</code> is not really the registry object in this library, but a proxy used to interact with one.</p>"},{"location":"quickstart/#exposition","title":"Exposition","text":"<p>To expose the metrics for prometheus to scrape use your favourite library!</p> <p>The library offers the <code>generate_metrics</code> function that will ask the global registry to collect all the metrics and return them to you in a text based format. From there is just a matter of exposing an endpoint (usually <code>/metrics</code>) where you return that data.</p> <pre><code>from pytheus.exposition import generate_metrics\n\nmetrics_data = generate_metrics()\n</code></pre> <p>It's good practice to set the <code>Content-Type</code> header so it's offered as a constant:</p> <pre><code>from pytheus.exposition import PROMETHEUS_CONTENT_TYPE\n\nheaders = {'Content-Type': PROMETHEUS_CONTENT_TYPE}\n</code></pre>"},{"location":"quickstart/#project","title":"Project","text":"<p>Now let's start with the quickstart project!</p>"},{"location":"quickstart/#setup","title":"Setup","text":"<p>First setup your environment.</p> <pre><code># create the project directory\nmkdir quickstart\ncd quickstart\n\n# create a python virtual environment and activate it\npython3 -m venv venv\n. venv/bin/activate\n\n# install the required libraries for the quickstart\npip install pytheus\npip install flask\n</code></pre>"},{"location":"quickstart/#api-service","title":"Api service","text":"<p>Next, let's build a service with two endpoints, one that will return a string normally and one that will be slowed artificially so that it will return the data with some delay.</p> quickstart.py<pre><code>import time\nfrom flask import Flask\n\napp = Flask(__name__)\n\n# normal endpoint\n@app.route('/')\ndef home():\n    return 'hello world!'\n\n# slowed endpoint with the `time` library\n@app.route('/slow')\ndef slow():\n    time.sleep(3)\n    return 'hello world! from slow!'\n\napp.run(host='0.0.0.0', port=8080)\n</code></pre> <p>You can now start the server with <pre><code>python quickstart.py\n</code></pre></p> <p>and you will be able to reach the <code>localhost:8080/</code> &amp; <code>localhost:8080/slow</code> endpoints for example from your browser or from a networking tool.</p> <p>The first one will return the string <code>hello world!</code> immediately while the second will take 3 seconds to return its string due to the <code>time.sleep(3)</code> call.</p>"},{"location":"quickstart/#adding-metrics","title":"Adding metrics","text":"<p>Now that we have a service with two endpoints we want to measure how much time it takes for them to respond. How many requests are handled in less than 1 second ? How many are handled over 5 seconds ? What is the 95th percentile ?</p> <p>We can answer those questions with metrics.</p> <p>First of all we import the metric class (<code>Histogram</code> makes sense for what we want to track):</p> <pre><code>from pytheus.metrics import Histogram\n</code></pre> <p>then we instantiate our metric, we need to give it a name and a documentation string explaining what the metric does.</p> <pre><code>http_request_duration_seconds = Histogram(\n    'http_request_duration_seconds', 'documenting the metric..'\n)\n</code></pre> <p>Note</p> <p>The metric name parameter passed in and the variable name don't have to be the same, here I've done it because it makes it easier to understand what a particular metric is observing.</p> <p>The documentation string is a mandatory parameter but can be the empty string <code>\"\"</code> although it would be better to document the metric.</p> <p>Now what is left to do is to use our metric to observe the endpoints, we can either use the <code>time()</code> method of the metric class or we can use the shortcut of using the metric as a decorator.</p> <p>We will show both approaches, one used for each endpoint:</p> track time with the context manager<pre><code>@app.route('/')\ndef home():\n    with http_request_duration_seconds.time():\n        return 'hello world!'\n</code></pre> track time with the decorator shortcut<pre><code>@app.route('/slow')\n@http_request_duration_seconds\ndef slow():\n    time.sleep(3)\n    return 'hello world! from slow!'\n</code></pre> <p>Congratulations! You have instrumented your service and you are now collecting metrics. \ud83c\udf89</p> <p>But wait, where are the metrics?</p>"},{"location":"quickstart/#exposing-the-metrics","title":"Exposing the metrics","text":"<p>With the code you've added the library is collecting metrics but now you need to expose them so that prometheus can scrape them.</p> <p>The way to do it is usually by having a <code>/metrics</code> endpoint with the data ready to be scraped, we can do that with the <code>generate_metrics</code> function.</p> <p>Let's create the endpoint exposing the metrics!</p> <p>We will start by importing the mentioned function:</p> <pre><code>from flask import Flask, Response  # we will need the Response class\nfrom pytheus.exposition import generate_metrics, PROMETHEUS_CONTENT_TYPE\n</code></pre> <p>Note</p> <p><code>PROMETHEUS_CONTENT_TYPE</code> is an helper containing the correct <code>Content-Type</code> for you to use as an header in your response.</p> <p>then we create the <code>/metrics</code> endpoint where we make use of the data generated by that function:</p> <pre><code>@app.route('/metrics')\ndef metrics():\n    data = generate_metrics()\n    return Response(data, headers={'Content-Type': PROMETHEUS_CONTENT_TYPE})\n</code></pre> <p>if you start the service and visit the <code>/metrics</code> endpoint you will see the collected data, and if you visit the other endpoints and then reload the metrics endpoint, you will see the updated data \ud83c\udf8a</p> localhost:8080/metrics<pre><code># HELP http_request_duration_seconds documenting the metric..\n# TYPE http_request_duration_seconds histogram\nhttp_request_duration_seconds_bucket{le=\"0.005\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"0.01\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"0.025\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"0.05\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"0.1\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"0.25\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"0.5\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"1\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"2.5\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"5\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"10\"} 0.0\nhttp_request_duration_seconds_bucket{le=\"+Inf\"} 0.0\nhttp_request_duration_seconds_sum 0.0\nhttp_request_duration_seconds_count 0.0\n</code></pre> <p>Tip</p> <p><code>0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10</code> are the default bucket values for the <code>Histogram</code> and can be changed!</p> <p>The final file should look like this:</p> quickstart.py<pre><code>import time\nfrom flask import Flask, Response\nfrom pytheus.metrics import Histogram\nfrom pytheus.exposition import generate_metrics, PROMETHEUS_CONTENT_TYPE\n\napp = Flask(__name__)\n\nhttp_request_duration_seconds = Histogram(\n    'http_request_duration_seconds', 'documenting the metric..'\n)\n\n@app.route('/metrics')\ndef metrics():\n    data = generate_metrics()\n    return Response(data, headers={'Content-Type': PROMETHEUS_CONTENT_TYPE})\n\n# track time with the context manager\n@app.route('/')\ndef home():\n    with http_request_duration_seconds.time():\n        return 'hello world!'\n\n# alternatively you can also track time with the decorator shortcut\n@app.route('/slow')\n@http_request_duration_seconds\ndef slow():\n    time.sleep(3)\n    return 'hello world! from slow!'\n\napp.run(host='0.0.0.0', port=8080)\n</code></pre>"},{"location":"quickstart/#loading-a-different-backend","title":"Loading a different Backend","text":"<p>The backend used by the metrics can be changed with the <code>load_backend</code> function. This changes where the information is stored and retrieved while leaving the api the same so that there is no difference between a single and a multiprocess use of the library.</p> <p>This library includes the <code>MultiProcessRedisBackend</code>, a Backend that makes use of Redis to support multi process python applications. If you prefer to use something different, you can create your own backend by respecting the <code>Backend</code> protocol.</p> <p>All we need to do to change from the <code>SingleProcessBackend</code>(used by default) to the <code>MultiProcessRedisBackend</code> is:</p> <pre><code>from pytheus.backends import load_backend\nfrom pytheus.backends.redis import MultiProcessRedisBackend\n\nload_backend(\n    backend_class=MultiProcessRedisBackend,\n    backend_config={\"host\": \"127.0.0.1\", \"port\": 6379},\n)\n</code></pre> <ul> <li><code>backend_class</code>: is the class respecting the <code>Backend</code> protocol that we want to use.</li> <li><code>backend_config</code>: is the configuration that you want to pass to the class. It's a dictionary.</li> </ul> <p>Tip</p> <p>It is also possible to define the values to be used with environment variables: <code>PYTHEUS_BACKEND_CLASS</code> &amp; <code>PYTHEUS_BACKEND_CONFIG</code></p> <p>That's it! By adding these lines our metrics are now making use of redis and will work with multiple processes :)</p>"},{"location":"registry/","title":"Registry","text":"<p>The <code>Registry</code> is the object that collects all of your metrics. By default there is a global registry called <code>REGISTRY</code> in the module <code>pytheus.registry</code>.</p> <p>When you create a new metric, it will be automatically added to this registry and when you generate metrics with <code>generate_metrics</code> it will use this global registry by default.</p> <p>Tip</p> <p>You can create a new metric that won't be added automatically to the global <code>REGISTRY</code> by passing the <code>registry</code> parameter:</p> <pre><code>counter = Counter('cache_hit_total', 'description', registry=None)\n</code></pre>"},{"location":"registry/#registry-protocol","title":"Registry Protocol","text":"<p>It is possible to create your own registry if you need, it just has to respect the <code>Registry</code> protocol:</p> <pre><code>class Registry(Protocol):\n    prefix: str | None\n\n    def register(self, collector: Collector) -&gt; None:\n        ...\n\n    def unregister(self, collector: Collector) -&gt; None:\n        ...\n\n    def collect(self) -&gt; Iterable:\n        ...\n</code></pre> <p>It has three required methods:</p> <ul> <li><code>register</code>: to register a new metric</li> <li><code>unregister</code>: to stop tracking a specific metric</li> <li><code>collect</code>: to collect all the samples from the registered metrics</li> </ul>"},{"location":"registry/#collectorregistry","title":"CollectorRegistry","text":"<p>The included registry in the library is a <code>CollectorRegistry</code>.</p> <p>Besides supporting the methods described in the protocol, it supports a <code>prefix</code> parameter when created that allows you to prefix all the metrics it collects.</p> <p>For example if you would have a metric like <code>http_request_duration_seconds</code> registered in a registry with prefix <code>service_a</code>, when generating metrics for scraping the output would be <code>service_a_http_request_duration_seconds</code>.</p> <p>Note</p> <p>Naming metrics like this is against prometheus best practices, the preferred approach would be to use labels instead of hardcoding the name in front of it.</p> <p>But it is possible that you might require this naming convention, maybe to have metrics for a specific service discoverable by starting to type the name so I feel the choice is up to the user.</p> <p>To create your own instance of a <code>CollectorRegistry</code> you would do:</p> <pre><code>from pytheus.registry import CollectorRegistry\n\nmy_registry = CollectorRegistry()\n</code></pre> <p>or if you want to have the prefix set:</p> <pre><code>my_registry = CollectorRegistry(prefix=\"service_a\")\n</code></pre> <p>To have metrics not register to the default global registry but to your new registry, you can pass it on creation:</p> <pre><code>my_registry = CollectorRegistry()\ncounter = Counter('cache_hit_total', 'description', registry=my_registry)\n</code></pre> <p>Tip</p> <p>You can also register metrics with the <code>register</code> method:</p> <pre><code>my_registry.register(counter)\n</code></pre> <p>If you didn't set the <code>registry</code> parameter when creating your metric it will still be added automatically to the default global registry unless you pass <code>registry=None</code>.</p> <p>Meaning that the metric would be registered on both the default global registry and your instantiated registry that you called <code>.register(counter)</code> on.</p> <p>and finally you can use the <code>generate_metrics</code> function with your own registry:</p> <pre><code>from pytheus.exposition import generate_metrics\n\nmy_registry = CollectorRegistry()\ngenerate_metrics(my_registry)\n</code></pre> <p>Note</p> <p>This becomes useful if you want multiple endpoints with different metrics, just create more registries and selectively add metrics to them and have different endpoints with the <code>generate_metrics</code> using each their own registry.</p>"},{"location":"registry/#registry-proxy","title":"Registry Proxy","text":"<p>The default global registry <code>REGISTRY</code> it's actually a <code>CollectorRegistryProxy</code>. A proxy created to make it easy to swap the default registry.</p> <p>It acts like a registry delegating the operations to the actual registry it holds.</p> <p>If you want to set the default global registry to an instance you created, for example with a prefix, you can do it like this:</p> <pre><code>from pytheus.registry import REGISTRY, CollectorRegistry\n\nmy_registry = CollectorRegistry(prefix='service_a')\nREGISTRY.set_registry(my_registry)\n</code></pre> <p>Warning</p> <p>This operation should be done before you create your metrics.</p>"},{"location":"rust_backend/","title":"Rust powered Backend \ud83e\udd80","text":"<p>\ud83e\uddea An experimental backend written in Rust is available for multiprocess support for both sync &amp; async applications.</p> <p>This removes the need for the <code>redis</code> library dependency &amp; allows the library to offer the same interface to either synchronous applications or asyncio based ones.</p> <p>Under the hood a different thread will handle the change of values &amp; pipeline requests, this makes it extremely fast to do operations like <code>inc()</code> on a metric in your application since it will always be handled asynchronously &amp; in parallel.</p>"},{"location":"rust_backend/#usage","title":"Usage","text":"<p>Installation:</p> <pre><code>pip install pytheus-backend-rs\n</code></pre> <p>then just load it like you would do with any other backends:</p> <pre><code>from pytheus_backend_rs import RedisBackend\n\nload_backend(\n    backend_class=RedisBackend,\n    backend_config={\"host\": \"127.0.0.1\", \"port\": 6379},\n)\n</code></pre> <p>Tip</p> <p>When calling <code>generate_metrics</code> it should be done in a <code>ThreadPool</code> for async applications. A framework like <code>FastAPI</code> does this automatically if your endpoint is defined as <code>def</code>. <pre><code>@app.get('/metrics')\ndef pytheus_metrics():\n    return generate_metrics()\n</code></pre></p> <p>The <code>GIL</code> gets released while waiting for the result so that other operations can run in parallel.</p>"},{"location":"rust_backend/#logs","title":"Logs","text":"<p>Logs will be emitted about threads initialization and errors if they happen. (ex. impossible to connect to redis)</p> <p>To see them just configure the python <code>logging</code> library, for example:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.INFO)\n</code></pre> <p>Note</p> <p>The logging needs to be setup before loading the backend with the <code>load_backend</code> function.</p> <pre><code>INFO:pytheus_backend_rs:Starting pipeline thread....0\nINFO:pytheus_backend_rs:Starting pipeline thread....1\nINFO:pytheus_backend_rs:Starting pipeline thread....2\nINFO:pytheus_backend_rs:Starting pipeline thread....3\nINFO:pytheus_backend_rs:Starting BackendAction thread....\nINFO:pytheus_backend_rs:RedisBackend initialized\n</code></pre>"},{"location":"rust_backend/#async","title":"Async","text":"<p>The Rust powered <code>RedisBackend</code> is safe to be used in <code>async</code> applications!</p> <p>The only detail is that for now it is preferable to use the <code>generate_metrics</code> function inside a <code>ThreadPool</code> for such applications.</p>"},{"location":"rust_backend/#performance-vs-python","title":"Performance vs Python \u26a1\ufe0f","text":"<p>The Rust backend library includes single process implementations as well. One equal to the Python implementation using Mutexes and one using Atomics.</p> <p>Info</p> <ul> <li><code>SingleProcessBackend</code> for the Mutex.</li> <li><code>SingleProcessAtomicBackend</code> for the Atomic.</li> </ul> <p>A simple benchmark with having a single <code>Counter</code> incrementing with <code>1 billion</code> iterations shows that there are performance improvements coming from just using the same implementation in <code>Rust</code>.</p> <p>Between the <code>Mutex</code> and <code>Atomic</code> approach the performance gain is small, with the atomics approach being slightly faster.</p> <p>1_000_000_000 iterations</p> Implementation Time taken (seconds) <code>Python Mutex</code> \ud83d\udc22 325~ <code>Rust Mutex</code> \ud83d\udc07 223~ <code>Rust Atomic</code> \ud83d\udc07 215~"},{"location":"summary/","title":"Summary","text":"<p>A <code>Summary</code> is a metric that captures individual observations and tracks the total size &amp; number of events observed. It can be useful to track latencies for example.</p> <p>When scraped a summary produces a couple of time series:</p> <ul> <li>total sum of observed values (ex. <code>http_request_duration_seconds_sum</code>)</li> <li>count of events that have been observed (ex. <code>http_request_duration_seconds_count</code>)</li> </ul> <p>Tip</p> <p>For when to use a summary vs histograms check the prometheus docs</p>"},{"location":"summary/#usage","title":"Usage","text":"<pre><code>from pytheus.metrics import Summary\n\nsummary = Summary(name=\"http_request_duration_seconds\", description=\"My description\")\n</code></pre>"},{"location":"summary/#observe-a-value","title":"Observe a value","text":"<p>To observe a value you can call the <code>observe()</code> method:</p> <pre><code>summary.observe(0.4)\n</code></pre>"},{"location":"summary/#track-time","title":"Track time","text":"<p>You can track time with the <code>time()</code> context manager, it will track the duration in seconds:</p> <pre><code>with summary.time():\n    do_something()\n</code></pre>"},{"location":"summary/#as-a-decorator","title":"As a Decorator","text":"<p>When used as a decorator the <code>Summary</code> will time the piece of code, syntactic sugar to the <code>time()</code> context manager:</p> <pre><code>@summary\ndef do_something():\n    ...\n</code></pre>"}]}